{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Quantization\n",
    "\n",
    "In this notebook we showcase how you try out different quantization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We showcase here the usage of our inference engine [Ofen](https://github/mixedbread-ai/ofen) for generating the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"ofen[torch]==0.0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from ofen.models import TextEncoder\n",
    "from ofen.enums import EncodingFormat\n",
    "from baguetter.indices import USearchDenseIndex\n",
    "from baguetter.evaluation import evaluate_retrievers, HFDataset\n",
    "\n",
    "# model_helpers also provides a stable implementation \n",
    "# of create_embed_fn using sentence-transformers\n",
    "from baguetter.utils.model_helpers import create_embed_fn_ofen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextEncoder.from_pretrained(\"mixedbread-ai/mxbai-embed-large-v1\")\n",
    "## Convert model to half precision (FP16) for efficiency\n",
    "model.half()\n",
    "\n",
    "# Define the embedding function expected by the USearchDenseIndex.\n",
    "# Alternatively, you can compute the embeddings yourself and add them to the index.\n",
    "# This function caches the float32 embeddings, to reuse them for different indices\n",
    "embed_fn = create_embed_fn_ofen(model, query_prompt=\"Represent this sentence for searching relevant passages: \", batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create different embedding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubinary_embed_fn = partial(embed_fn, encoding_format=EncodingFormat.UBINARY)\n",
    "# Not supported atm\n",
    "# binary_embed_fn = partial(embed_fn, encoding_format=EncodingFormat.BINARY)\n",
    "# Not supported atm\n",
    "# uint8_embed_fn = partial(embed_fn, encoding_format=EncodingFormat.UINT8)\n",
    "int8_embed_fn = partial(embed_fn, encoding_format=EncodingFormat.INT8)\n",
    "float32_embed_fn = partial(embed_fn, encoding_format=EncodingFormat.FLOAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating  3 retrievers...\n",
      "---------------------------------------------------------------\n",
      "Datasets:  ['mteb/scidocs']\n",
      "Top K:  100\n",
      "Metrics:  ['ndcg@1', 'ndcg@5', 'ndcg@10', 'precision@1', 'precision@5', 'precision@10', 'mrr@1', 'mrr@5', 'mrr@10']\n",
      "Ignore identical IDs:  True\n",
      "\n",
      "Evaluating Dataset: mteb/scidocs\n",
      "---------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Adding 25657 documents to ubinary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add: 100%|██████████| 25657/25657 [00:00<00:00, 50087.37vector/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 25657 documents to ubinary took 1.10 seconds\n",
      "Starting Searching 1000 queries with ubinary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Search: 100%|██████████| 1000/1000 [00:00<00:00, 28258.17vector/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching 1000 queries with ubinary took 0.22 seconds\n",
      "Starting Adding 25657 documents to int8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add: 100%|██████████| 25657/25657 [00:00<00:00, 41737.65vector/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 25657 documents to int8 took 1.21 seconds\n",
      "Starting Searching 1000 queries with int8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Search: 100%|██████████| 1000/1000 [00:00<00:00, 11246.50vector/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching 1000 queries with int8 took 0.26 seconds\n",
      "Starting Adding 25657 documents to float32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add: 100%|██████████| 25657/25657 [00:00<00:00, 52296.59vector/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 25657 documents to float32 took 1.08 seconds\n",
      "Starting Searching 1000 queries with float32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Search: 100%|██████████| 1000/1000 [00:00<00:00, 4579.06vector/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching 1000 queries with float32 took 0.40 seconds\n",
      "\n",
      "Report (rounded):\n",
      "---------------------------------------------------------------\n",
      "#    Model      NDCG@1  NDCG@5    NDCG@10      P@1  P@5     P@10      MRR@1  MRR@5    MRR@10\n",
      "---  -------  --------  --------  ---------  -----  ------  ------  -------  -------  --------\n",
      "a    ubinary     0.251  0.175     0.211      0.251  0.153   0.110     0.251  0.343    0.359\n",
      "b    int8        0.236  0.187ᵃ    0.228ᵃ     0.236  0.170ᵃ  0.122ᵃ    0.236  0.351    0.365\n",
      "c    float32     0.253  0.193ᵃ    0.231ᵃ     0.253  0.174ᵃ  0.121ᵃ    0.253  0.364ᵃ   0.377ᵃ\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the retrievers\n",
    "datasets = [HFDataset(\"mteb/scidocs\")]\n",
    "\n",
    "# Evaluate dense retriever\n",
    "result = evaluate_retrievers(\n",
    "    datasets=datasets,\n",
    "    retriever_factories={\n",
    "        \"ubinary\": lambda: USearchDenseIndex(\n",
    "            embedding_dim=model.embedding_dim,\n",
    "            embed_fn=ubinary_embed_fn,\n",
    "            metric=\"hamming\",\n",
    "        ),\n",
    "        \"int8\": lambda: USearchDenseIndex(\n",
    "            embedding_dim=model.embedding_dim,\n",
    "            embed_fn=int8_embed_fn,\n",
    "            dtype=np.int8\n",
    "        ),\n",
    "        \"float32\": lambda: USearchDenseIndex(\n",
    "            embedding_dim=model.embedding_dim,\n",
    "            embed_fn=float32_embed_fn,\n",
    "        )\n",
    "    }\n",
    ")\n",
    "result.save(\"eval_results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
